import io
import os
import time
import base64
import shutil
import requests
import subprocess
from PIL import Image
from openai import OpenAI
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage
from datetime import datetime
from dotenv import load_dotenv

class ScreenSpeak:
    """
    ScreenSpeak continuously monitors a specified directory for new screenshots,
    processes each new screenshot by generating a voice-over script using OpenAI's GPT model,
    and then synthesizes speech from the script.
    """
    def __init__(self, screenshot_dir, voice_model="alloy", poll_interval=10):
        """
        Initializes the ScreenSpeak with the directory to monitor, voice model, and polling interval.

        :param screenshot_dir: Directory to monitor for new screenshots.
        :param voice_model: Voice model to use for speech synthesis.
        :param poll_interval: Time in seconds between directory checks for new screenshots.
        """
        self.screenshot_dir = screenshot_dir
        self.voice_model = voice_model
        self.poll_interval = poll_interval
        self.start_time = datetime.now().timestamp()
        self.last_processed = None
        self.client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
        self.anthropic_client = ChatAnthropic(temperature=0, anthropic_api_key=os.getenv("ANTHROPIC_API_KEY"), model_name="claude-3-opus-20240229")

        # Base directory for all outputs
        base_output_dir = os.path.join(os.path.dirname(__file__), "LocalScreenSpeakOutputs")
        # Specific directories for text analysis and screenshots
        self.text_analysis_output_dir = os.path.join(base_output_dir, "Text Analysis")
        self.screenshot_output_dir = os.path.join(base_output_dir, "Screenshots")

        # Create the directories if they don't exist
        os.makedirs(self.text_analysis_output_dir, exist_ok=True)
        os.makedirs(self.screenshot_output_dir, exist_ok=True)
    
    def run(self):
        """
        Starts the monitoring and processing of new screenshots in the specified directory.
        """
        print(f"Monitoring for new screenshots in {self.screenshot_dir}...")
        while True:
            latest_screenshot = self._get_latest_screenshot()
            if latest_screenshot and latest_screenshot != self.last_processed:
                self._process_screenshot(latest_screenshot)
                self.last_processed = latest_screenshot
            time.sleep(self.poll_interval)

    def _get_latest_screenshot(self):
        """
        Gets the most recent screenshot file from the directory, considering only files created after the script started.

        :return: The path to the latest screenshot file, or None if no new file is found.
        """
        png_files = [f for f in os.listdir(self.screenshot_dir) if f.endswith('.png')]
        valid_files = [f for f in png_files if os.path.getctime(os.path.join(self.screenshot_dir, f)) > self.start_time]
        if not valid_files:
            return None
        latest_file = max(valid_files, key=lambda x: os.path.getctime(os.path.join(self.screenshot_dir, x)))
        return os.path.join(self.screenshot_dir, latest_file)

    def _copy_screenshot(self, screenshot_path, file_name_base):
        new_filename = f"{file_name_base}.png"
        destination_path = os.path.join(self.screenshot_output_dir, new_filename)
        shutil.copy2(screenshot_path, destination_path)
        print(f"Copied screenshot to {destination_path}")

    def _synthesize_best_answer(self, openai_description, anthropic_description):
        """
        Synthesizes the best answer from the descriptions generated by OpenAI and Anthropic models.

        :param openai_description: Description generated by OpenAI's model.
        :param anthropic_description: Description generated by Anthropic's model.
        :return: Synthesized best answer as a string.
        """
        client = OpenAI()
        combined_prompt = f"Please consider both of these answers and synthesize a composite best answer:\n\nOpenAI Description:\n{openai_description}\n\nAnthropic Description:\n{anthropic_description}"
        response = client.chat.completions.create(
            model="gpt-4-0125-preview",
            messages=[
                {"role": "system", "content": "You are designed to consider two answers from different artificial intelligence LLMs and generate the best composite synthesized answeer from the original two answer."},
                {"role": "user", "content": combined_prompt}
                ],
            temperature=0.7,
        )
        return response.choices[0].message

    def _save_text_analysis(self, synthesized_description, screenshot_path):
        """
        Saves the synthesized text analysis to a file in the "LocalScreenSpeakOutputs/Text Analysis" folder,
        using a filename generated from the description.
    
        :param synthesized_description: The synthesized description to save, which should be a string.
        :param screenshot_path: The path of the original screenshot, used to derive the name of the text file.
        """
        # Generate a filename based on the synthesized description
        filename = self._generate_file_name(synthesized_description)
        text_file_path = os.path.join(self.text_analysis_output_dir, f"{filename}.txt")
        
        # Specify the new base directory relative to the current script location
        base_dir = os.path.dirname(__file__)  # Gets the directory where the script is located
        
        # Adjust the path to point to "LocalScreenSpeakOutputs/Text Analysis" relative to the script's location
        text_analysis_dir = os.path.join(base_dir, "LocalScreenSpeakOutputs", "Text Analysis")
        
        if not os.path.exists(text_analysis_dir):
            os.makedirs(text_analysis_dir)
        
        # Define the full path for the text file within the "Text Analysis" directory
        text_file_path = os.path.join(text_analysis_dir, f"{filename}.txt")
        
        # Check and handle if synthesized_description is not a string
        if not isinstance(synthesized_description, str):
            # Assuming synthesized_description has a property 'content' containing the text
            content_to_write = synthesized_description.content
        else:
            content_to_write = synthesized_description
        
        # Write the synthesized description to the text file
        with open(text_file_path, 'w', encoding='utf-8') as file:
            file.write(content_to_write)
        
        print(f"Saved text analysis to {text_file_path}")

    def _generate_file_name(self, description):
        """
        Generates a file name based on the provided description using ChatGPT.

        :param description: The script or summary of the script.
        :return: A suitable file name as a string.
        """
        messages = [{
            "role": "system",
            "content": "You are a file name generator. Using a brief description of a screenshot, you are going to create appropriate file names."
        }, {
            "role": "user",
            "content": f"Generate a concise, descriptive file name based on the following summary:\n{description}"
        }]

        result = self.client.chat.completions.create(
            model="gpt-4-0125-preview",  # Ensure you're using a valid model identifier
            messages=messages,
            max_tokens=100,
            temperature=0.7
        )

        if result.choices and len(result.choices) > 0:
            file_name = result.choices[0].message.content.strip()
        else:
            file_name = "default_filename"

        # Replace any illegal characters in file name
        file_name = "".join([c for c in file_name if c.isalnum() or c in [' ', '_', '-']]).rstrip()

        return file_name

    def _generate_script(self, screenshot_path):
        """
        Generates a voice-over script for the screenshot using OpenAI's GPT model.

        :param screenshot_path: Path to the screenshot file.
        :return: Generated script as a string.
        """
        with Image.open(screenshot_path) as img:
            img = img.convert("RGB")
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG", quality=85)
            base64_image = base64.b64encode(buffered.getvalue()).decode('utf-8')
        
        prompt_messages = [{
            "role": "user",
            "content": [
                "This is a screenshot. Based on the image please generate the text to describe what you see. Try your best.",
                {"image": base64_image, "resize": 768}
            ],
        }]
        result = self.client.chat.completions.create(
            model="gpt-4-vision-preview",
            messages=prompt_messages,
            max_tokens=500,
        )
        return result.choices[0].message.content
    
    def _generate_script_anthropic(self, screenshot_path):
        """
        Generates a voice-over script for the screenshot using Anthropic's Claude-3 model.

        :param screenshot_path: Path to the screenshot file.
        :return: Generated script as a string.
        """
        with Image.open(screenshot_path) as img:
            img = img.convert("RGB")
            buffered = io.BytesIO()
            img.save(buffered, format="JPEG", quality=85)
            base64_image = base64.b64encode(buffered.getvalue()).decode('utf-8')

        # Prepare the message for Anthropic's API using HumanMessage
        messages = [
            HumanMessage(
                content=[
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{base64_image}",
                        },
                    },
                    {"type": "text", "text": "This is a screenshot. Based on the image please generate the text to describe what you see. Try your best."},
                ]
            )
        ]

        # Assuming `self.anthropic_client` is already initialized and configured
        response = self.anthropic_client.invoke(messages)

        return response    

    def _synthesize_speech(self, script):
        """
        Synthesizes speech from the provided script using the specified voice model.

        :param script: Script to synthesize speech from.
        :return: Binary content of the synthesized speech audio.
        """
        print("Synthesizing speech...")
        response = requests.post(
            "https://api.openai.com/v1/audio/speech",
            headers={"Authorization": f"Bearer {os.getenv('OPENAI_API_KEY')}"},
            json={"model": "tts-1-hd", "input": script, "voice": self.voice_model},
        )
        if response.ok:
            return response.content
        else:
            print(f"Error with the audio generation request: {response.text}")
            return None

    def _save_and_play_audio(self, audio_content, identifier):
        """
        Saves the synthesized speech audio to a file and plays it.

        :param audio_content: Binary content of the synthesized speech audio.
        :param identifier: A string identifier to differentiate the source of the audio content.
        """
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        audio_filename = f"{identifier}_audio_{timestamp}.mp3"
        with open(audio_filename, 'wb') as audio_file:
            audio_file.write(audio_content)
        print(f"Saved audio to {audio_filename}")
        subprocess.run(['mpg123', audio_filename], check=True)

if __name__ == "__main__":
    load_dotenv()
    screenspeak = ScreenSpeak("/mnt/c/Users/ptcap/OneDrive/Pictures/Screenshots")
    screenspeak.run()
